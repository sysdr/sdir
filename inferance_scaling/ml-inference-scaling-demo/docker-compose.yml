version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: ml-inference-redis
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  ml-inference:
    build: .
    container_name: ml-inference-app
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - ./app:/app/app
      - ./frontend:/app/frontend

  load-tester:
    build: .
    container_name: ml-inference-tester
    command: python tests/load_test.py --requests 50 --concurrency 5
    depends_on:
      ml-inference:
        condition: service_healthy
    environment:
      - REDIS_HOST=redis
    profiles:
      - testing
